---
layout: home
title: Home
---

Hi! I am a Research Scientist at <a href="https://research.fb.com/category/facebook-ai-research/">Facebook AI Research (FAIR)</a> in New York, broadly interested in computer vision and machine learning. 

I recently graduated with my PhD in Computer Science from the School of Interactive Computing at Georgia Tech.
My PhD advisor was <a href='https://www.cc.gatech.edu/~parikh/'>Devi Parikh</a>.
My thesis was on "Interpretation, Grounding and Imagination for Machine Intelligence".

For more details on my research see my <a href='https://scholar.google.com/citations?user=v1CRzeAAAAAJ&hl=en'>Google Scholar</a> page or checkout my <a href="http://vrama91.github.io/publications/">publications</a>. During my PhD, my research was supported by a Google PhD fellowship in Machine Perception, Speech Technology and Computer Vision.

[//]: # "On the vision side, I am interested in problems in vision and language, learning common sense and visual reasoning. On the machine learning side, I am interested in developing tools for effective low-shot learning, generative models, bayesian deep learning and variational inference."

[//]: # "I also care about issues of how we evaluate our models, as we edge towards higher-level AI-complete tasks. In my first project in grad school, I worked on a (now popularly used) evaluation metric for image captioning called CIDEr."

I have been fortunate to work with some great mentors and collaborators during grad school, including <a href="http://larryzitnick.org/">Larry Zitnick</a>,
<a href="http://www.cc.gatech.edu/~dbatra/index.html">Dhruv Batra</a>,
<a href="https://www.cs.ubc.ca/~murphyk/">Kevin Murphy</a>,
<a href="http://ai.stanford.edu/~gal/">Gal Chechik</a>, and <a href="http://bengio.abracadoudou.com/">Samy Bengio</a>.

In a previous life, I was an undergrad in ECE at IIIT-Hyderabad where I worked with <a href='http://www.iiit.ac.in/people/faculty/mkrishna'>K. Madhava Krishna</a> in Robotics. <a href='https://sites.google.com/site/ramakrishnavedantam928/'>Here</a> is a link to my old website.

<hr/>

<h3>News</h3>
<ul>
<li> <b>[January, 2019]</b> I started at<a href="https://research.fb.com/category/facebook-ai-research/">Facebook AI Research (FAIR)</a> as a Research Scientist!</li>
<li> <b>[April, 2018]</b> I received the <a href="https://research.googleblog.com/2018/04/announcing-2018-google-phd-fellows-for.html">2018 Google PhD Fellowship in Machine Perception, Speech Technology and Computer Vision</a>!</li>
<li> <b>[March, 2018]</b> I will be interning at MSR Cambridge this summer, working on generative models of vision, language and action!</li>
<li> <b>[Feb, 2018]</b> Paper on learning grounded generative (image) models accepted to ICLR, 2018!</li>
<li> <b>[August, 2017]</b> I will be moving to Georgia Tech this Fall, following my advisor's move!</li>
<li> <b>[June, 2017]</b> Recognized as one of the outstanding reviewers at CVPR, 2017</li>
<li> <b>[June, 2017]</b> Paper on learning word embeddings grounded in sounds accepted as a short paper at EMNLP, 2017!</li>
<li> <b>[May, 2017]</b> I interned at Facebook AI Research (FAIR) in Summer, 2017, working with <a href='http://filebox.ece.vt.edu/~parikh'>Devi Parikh</a>, <a href="http://www.cc.gatech.edu/~dbatra/index.html">Dhruv Batra</a> and <a href="http://rohrbach.vision/">Marcus Rohrbach</a>!</li>
<li> <b>[March, 2017]</b> Two papers accepted to CVPR 2017 as Spotlight presentations! </li>
<li> <b>[Jan, 2017]</b> I interned at Google Research in Winter 2017, working with <a href="http://research.google.com/pubs/KevinMurphy.html">Kevin Murphy</a> on generative models for images!</li>
<li> <b>[May, 2016]</b> I interned at Google Research in Summer 2016, with <a href="http://ai.stanford.edu/~gal/">Gal Chechik</a> and <a href="http://bengio.abracadoudou.com/">Samy Bengio</a>!</li>
</ul>
<hr/>

<h3>Code</h3>
<ul>
<li> MSCOCO Caption Evaluation <a href="https://github.com/tylin/coco-caption"> code</a></li>
<li> <a href="https://github.com/vrama91/coco-caption">Codes</a> from MSCOCO Caption Evaluation for metrics (BLEU, ROUGE, CIDEr-D and METEOR), independent of the COCO annotations </li>
<li> Code for our CVPR'16 paper on <a href="https://github.com/satwikkottur/VisualWord2Vec">Learning Visually Grounded Word Embeddings</a></li>
<li> Code for our ICLR'18 paper on <a href="https://github.com/google/joint_vae">Generative Models of Visually Grounded Imagination</a></li>
</ul>	
<hr/>
If you like this layout/page, see <a href='demo-post'>this</a> to build your own using github+jekyll 
